{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fdc2f3-287e-4862-8c44-e92447c8dcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. What is an ensemble technique in machine learning?\n",
    "Ans.\n",
    "An ensemble technique in machine learning involves combining multiple individual models to create a stronger\n",
    "and more robust predictive model. The idea behind ensemble methods is that by combining the predictions of \n",
    "multiple models, the overall performance can be better than that of any individual model. This can lead to\n",
    "improved generalization, better accuracy, and increased stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9ad14c-12c2-4dea-855c-7c27a6cff261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2. Why are ensemble techniques used in machine learning?\n",
    "Ans.\n",
    "The ensemble techniques used in machine learning.\n",
    "1. Improved Accuracy: Ensemble methods can often achieve higher accuracy than individual models. By combining\n",
    "the predictions of multiple models, the ensemble can better capture complex patterns in the data, leading to \n",
    "improved generalization.\n",
    "\n",
    "2. Reduction of Overfitting: Ensemble techniques, especially bagging, help reduce overfitting by averaging or\n",
    "combining predictions from multiple models. This is particularly beneficial when dealing with high-variance \n",
    "models that may perform well on the training data but poorly on new, unseen data.\n",
    "\n",
    "3. Handling Different Aspects of Data: Different models may excel at capturing different aspects or patterns\n",
    "within the data. Ensemble techniques allow combining these strengths, leading to a more comprehensive \n",
    "understanding of the underlying relationships in the data.\n",
    "\n",
    "4. Increased Stability: Ensemble methods tend to provide more stable and reliable predictions. This is crucial\n",
    "in scenarios where small changes in the training data can lead to significant variations in the model's output.\n",
    "\n",
    "5. Easy Parallelization: Some ensemble methods, like bagging, are highly parallelizable. Training individual \n",
    "models can be done independently, making ensembles suitable for distributed computing environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3ca456-a5a0-4cb6-9b6d-34b73b66ca68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3. What is bagging?\n",
    "Ans. \n",
    "Bagging involves training multiple instances of the same learning algorithm on different subsets of the training\n",
    "data. The final prediction is often an average (for regression) or a vote (for classification) of the predictions\n",
    "from individual models.Bagging aims to reduce overfitting and variance by combining diverse models trained on \n",
    "different subsets of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7cac1f-2308-4780-88af-a73b96379ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4. What is boosting?\n",
    "Ans.\n",
    "Boosting focuses on training multiple weak learners (models slightly better than random chance) sequentially. Each\n",
    "subsequent model gives more weight to instances that were misclassified by the previous models.Boosting aims to\n",
    "improve accuracy by iteratively emphasizing difficult-to-learn examples, leading to a strong, robust predictive \n",
    "model. Popular algorithms include AdaBoost, Gradient Boosting (e.g., XGBoost, LightGBM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a0b6a3-8eec-4ff5-8773-96c39aa21616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5. What are the benefits of using ensemble techniques?\n",
    "Ans.\n",
    "Ensemble techniques offer several benefits in machine learning:\n",
    "1. Improved Accuracy: Ensembles often achieve higher accuracy than individual models by combining the strengths of \n",
    "multiple models, capturing a broader range of patterns in the data.\n",
    "\n",
    "2. Reduction of Overfitting: Ensembles, especially bagging, help reduce overfitting by averaging or combining predictions\n",
    "from multiple models. This is beneficial when dealing with high-variance models that may perform well on training data \n",
    "but poorly on new data.\n",
    "\n",
    "3. Enhanced Robustness: Ensembles are more robust to outliers and noise in the data. Individual models might make errors on\n",
    "certain instances, but by aggregating predictions, the impact of these errors is mitigated.\n",
    "\n",
    "4. Easy Parallelization: Some ensemble methods, like bagging, are highly parallelizable. Training individual models can be\n",
    "done independently, making ensembles suitable for distributed computing environments.\n",
    "\n",
    "5. Ensemble Diversity: Ensuring diversity among individual models in an ensemble is crucial. Different models may make different\n",
    "errors, and combining them can help compensate for individual weaknesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dcc183-6232-40fe-9954-92b8de2a715e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6. Are ensemble techniques always better than individual models?\n",
    "Ans. \n",
    "Ensemble techniques are not always better than individual models, but they often outperform single models, especially when \n",
    "dealing with complex or noisy data. The effectiveness of ensembles depends on factors such as the diversity of base models\n",
    ", the quality of individual models, and the nature of the data. Ensembles shine in situations where different models capture\n",
    "different aspects of the underlying patterns, leading to improved overall performance. However, in simpler or well-behaved\n",
    "datasets, where a single strong model may suffice, the benefits of ensembles might be less pronounced. It's essential to \n",
    "consider the specific characteristics of the problem at hand when deciding whether to use ensemble techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242c2446-2c31-4c86-af00-8d2cb8921bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7. How is the confidence interval calculated using bootstrap?\n",
    "Ans.\n",
    "Calculating a confidence interval using bootstrap involves the following steps:\n",
    "\n",
    "1. Data Resampling:\n",
    "Start with your original sample data. Generate multiple bootstrap samples by randomly sampling with replacement from the \n",
    "original data. Each bootstrap sample should have the same size as the original dataset.\n",
    "\n",
    "2. Statistic Calculation:\n",
    "For each bootstrap sample, calculate the statistic of interest. In the case of estimating the population mean, this would be the\n",
    "sample mean.\n",
    "\n",
    "3. Create Bootstrap Distribution:\n",
    "Create a distribution of the calculated statistics from the bootstrap samples.\n",
    "\n",
    "4. Percentile Method:\n",
    "Determine the confidence interval by finding the desired percentiles of the bootstrap distribution.\n",
    "For a 95% confidence interval, you would typically use the 2.5th percentile as the lower bound and the 97.5th percentile as the\n",
    "upper bound.\n",
    "\n",
    "Let's say we have a sample of 50 tree heights with a mean of 15 meters and a standard deviation of 2 meters.\n",
    "\n",
    "Now, perform the following steps:\n",
    "1. Data Resampling:\n",
    "Generate, let's say, 1000 bootstrap samples by randomly selecting 50 tree heights with replacement from the original sample.\n",
    "\n",
    "2. Statistic Calculation:\n",
    "Calculate the mean for each of the 1000 bootstrap samples.\n",
    "\n",
    "3. Create Bootstrap Distribution:\n",
    "You now have a distribution of 1000 sample means from the bootstrap samples.\n",
    "\n",
    "4. Percentile Method:\n",
    "Determine the 95% confidence interval by finding the 2.5th and 97.5th percentiles of the bootstrap distribution.\n",
    "\n",
    "5. The resulting confidence interval would be the range of values from the lower bound to the upper bound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3686942-fabb-40f1-b4dd-d5f93a3f7373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q8. How does bootstrap work and What are the steps involved in bootstrap?\n",
    "Ans.\n",
    "Bootstrap is a resampling technique used to estimate the sampling distribution of a statistic by repeatedly \n",
    "resampling with replacement from the observed data. \n",
    "\n",
    "Here are the steps involved in the bootstrap procedure:\n",
    "    \n",
    "1. Start with your original dataset, which consists of observations or measurements.\n",
    "2. Generate multiple bootstrap samples by randomly selecting data points from the original dataset with replacement.\n",
    "Each bootstrap sample should have the same size as the original dataset, but individual data points may be repeated.\n",
    "3. For each bootstrap sample, calculate the statistic of interest. This could be the mean, median, standard deviation,\n",
    "or any other relevant statistic.\n",
    "4. Repeat steps 2 and 3 a large number of times (e.g., 1000 or more) to create a distribution of the calculated statistics.\n",
    "Estimate of Sampling Distribution:\n",
    "\n",
    "The distribution of these calculated statistics forms an empirical approximation of the sampling distribution of the statistic of interest.\n",
    "Confidence Interval:\n",
    "\n",
    "Determine the confidence interval for the statistic by finding the desired percentiles of the bootstrap distribution.\n",
    "For example, a 95% confidence interval would be determined by the 2.5th and 97.5th percentiles of the bootstrap distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d24c1cdb-d4af-44c9-94d3-9c840cb2b306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample mean height for 50 Trees is 15 and Sample Standard Deviation is 2\n",
      "T-Statistic with 95.0% condifence interval for dof 49 : 2.0096\n",
      "Standard Error : 0.2828\n",
      "Margin of error : 0.5684\n",
      "\n",
      "Estimated Population mean with 95% confidence interval is (14.43 , 15.57)\n"
     ]
    }
   ],
   "source": [
    "# Q9. A researcher wants to estimate the mean height of a population of trees. They measure the height of a\n",
    "# sample of 50 trees and obtain a mean height of 15 meters and a standard deviation of 2 meters. Use\n",
    "# bootstrap to estimate the 95% confidence interval for the population mean height.\n",
    "\n",
    "# Given Data \n",
    "samples = 50\n",
    "sample_mean = 15\n",
    "sample_std = 2\n",
    "confidence_level = 0.95\n",
    "\n",
    "# Calculate the t value for desired level of confidence\n",
    "import scipy.stats as stats\n",
    "alpha = 1 - confidence_level\n",
    "dof = samples-1\n",
    "t_value = stats.t.ppf(1 - alpha/2, dof)\n",
    "\n",
    "# calculate the standard error and margin of error\n",
    "import math\n",
    "std_error = sample_std / math.sqrt(samples)\n",
    "margin_of_error = t_value * std_error\n",
    "\n",
    "# calculate the confidence interval bounds\n",
    "lower_bound = sample_mean - margin_of_error\n",
    "upper_bound = sample_mean + margin_of_error\n",
    "\n",
    "# print 95% confidence interval\n",
    "print(f'Sample mean height for {samples} Trees is {sample_mean} and Sample Standard Deviation is {sample_std}')\n",
    "print(f'T-Statistic with {confidence_level*100}% condifence interval for dof {dof} : {t_value:.4f}')\n",
    "print(f'Standard Error : {std_error:.4f}')\n",
    "print(f'Margin of error : {margin_of_error:.4f}')\n",
    "print(f'\\nEstimated Population mean with 95% confidence interval is ({lower_bound:.2f} , {upper_bound:.2f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224bd6d7-2283-4b56-9505-6caaf75b15c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
